<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="Hugo 0.126.1">

  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="author" content="Boxcounter" />
  <meta property="og:url" content="http://boxcounter.com/posts/2024-06-20-residual-block-intuitive-understanding/" />
  <link rel="canonical" href="http://boxcounter.com/posts/2024-06-20-residual-block-intuitive-understanding/" /><script type="application/ld+json">
  {
      "@context" : "http://schema.org",
      "@type" : "BlogPosting",
      "mainEntityOfPage": {
           "@type": "WebPage",
           "@id": "http:\/\/boxcounter.com\/"
      },
      "articleSection" : "posts",
      "name" : "Residual block 的直观理解",
      "headline" : "Residual block 的直观理解",
      "description" : "这两天在学习残差网络（ResNet），其中的核心组件 —— 残差块（Residual block）的设计原理让我困惑了好久。在查阅了不少资料、向 ChatGPT",
      "inLanguage" : "en-US",
      "author" : "Boxcounter",
      "creator" : "Boxcounter",
      "publisher": "Boxcounter",
      "accountablePerson" : "Boxcounter",
      "copyrightHolder" : "Boxcounter",
      "copyrightYear" : "2024",
      "datePublished": "2024-06-20 00:00:00 \u002b0000 UTC",
      "dateModified" : "2024-06-20 00:00:00 \u002b0000 UTC",
      "url" : "http:\/\/boxcounter.com\/posts\/2024-06-20-residual-block-intuitive-understanding\/",
      "keywords" : [  ]
  }
</script>
<title>Residual block 的直观理解 - Boxcounter&#39;s blog</title>
  <meta property="og:title" content="Residual block 的直观理解 - Boxcounter&#39;s blog" />
  <meta property="og:type" content="article" />
  <meta name="description" content="这两天在学习残差网络（ResNet），其中的核心组件 —— 残差块（Residual block）的设计原理让我困惑了好久。在查阅了不少资料、向 ChatGPT" />

  <link rel="stylesheet" href="/css/flexboxgrid-6.3.1.min.css" />
  <link rel="stylesheet"
    href="/css/github-markdown.min.css" />
  <link rel="stylesheet" href="/css/highlight/tomorrow.min.css" />
  <link rel="stylesheet" href="/css/index.css">
  <link href="/index.xml" rel="alternate" type="application/rss+xml" title="Boxcounter&#39;s blog">
  
  <link href="https://fonts.googleapis.com/css?family=Arvo|Permanent+Marker" rel="stylesheet">
  
  

  
</head>


<body>
  <article class="post " id="article">
    <div class="row">
      <div class="col-xs-12 col-sm-10 col-md-8 col-sm-offset-1 col-md-offset-2 col-lg-6 col-lg-offset-3">
        <div class="site-header">
          
<header>
  <div class="signatures site-title">
    <a href="/">Boxcounter</a>
  </div>
</header>
<div class="row end-xs">
  
  
</div>
<div class="header-line"></div>

        </div>
        <header class="post-header">
          <h1 class="post-title">Residual block 的直观理解</h1>
          
          <div class="row post-desc">
            <div class="col-xs-6">
              
              <time class="post-date" datetime="2024-06-20 00:00:00 UTC">
                20 Jun 2024
              </time>
              
            </div>
            <div class="col-xs-6">
              
              <div class="post-author">
                <a target="_blank" href="http://boxcounter.com/">@Boxcounter</a>
              </div>
              
            </div>
          </div>
          
        </header>

        <div class="post-content markdown-body">
          <p>这两天在学习残差网络（ResNet），其中的核心组件 —— 残差块（Residual block）的设计原理让我困惑了好久。在查阅了不少资料、向 ChatGPT 反复请教后，我有了一些直观的理解，整理如下。</p>
<h2 id="一残差residual是什么">一、残差（Residual）是什么？</h2>
<p>简单来说，残差是 prediction 和 input 之间的差值。这个定义本身比较简单，但其原理却不那么容易理解。因为它涉及到转变我思考问题的方式。</p>
<p>具体来说，在引入这个概念之前，深度学习中的神经网络主要关注的是另一个差值 loss，也就是 prediction 和 truth 之间的差值。通过反向传播和梯度下降，神经网络总能从 loss 中学习，性能越来越好。这个思路比较直观：loss 就是误差，神经网络通过不断降低 loss 来提升自身的性能。那残差的含义是啥？百思不得其解。</p>
<p>除此之外，研究者还发现，增加神经网络的深度可以提升其性能。从直觉上来理解，这类似于脑容量变大和大脑皮层神经元变多后，学习能力会增强。</p>
<p>于是，神经网络越来越深，从最初的几层演变为几十层。一切都挺好且充满希望。直到……</p>
<h2 id="二残差要解决什么问题">二、残差要解决什么问题？</h2>
<p>随着神经网络的层数继续增多，研究者发现神经网络的性能反而变差了。</p>
<p>主要原因是随着层数增多，梯度消失或爆炸（vanishing/exploding gradient）问题越来越常出现，使得神经网络越来越难以从 loss 这个差值中学到有用的知识，性能也因此下降。</p>
<p>换句话说，增加的层数帮了倒忙。</p>
<h2 id="三残差怎么解决这个问题">三、残差怎么解决这个问题？</h2>
<p>设计者抱着一个朴素的愿望给添加的层设定了一个底线 —— “别添乱”。然后期望在守住这个底线的同时，它们学点有用的知识。</p>
<p>手段就是用残差块替代传统的层。</p>
<p>残差块内部分为残差连接（residual connection）和残差映射（residual mapping）。它俩的特征分别是：</p>
<ol>
<li>残差连接：无参数且不学习，它的工作是把 input 原封不动地传递给下一层。</li>
<li>残差映射：有参数且能学习，它像传统的层一样从 loss 中努力地学点什么。</li>
</ol>
<p>那它们是怎么守住“不添乱”这个底线呢？</p>
<p>靠的是残差映射的学习能力和残差连接的兜底。具体来说是这样：</p>
<ul>
<li>如果残差映射始终无法从 loss 中学到东西，即无论怎样 loss 都没有减小，那么残差映射最终会学习让自己啥都不干，只输出 0。这种情况下，整个残差块就靠残差连接一个单元干活了，相当于一个复读机 —— 把 input 原封不动地输出给下一层。</li>
<li>如果在守住这个底线之后，残差映射还能学到一些知识，残差块就会提升神经网络的表现。</li>
</ul>
<p>打个比方，继承家业后，二代一边沿用已有的经营方法（残差连接），另一边引入数字化和智能化（残差映射）谋发展。</p>
<ul>
<li>如果数字化和智能化开花结果，企业就上一个台阶 —— 残差映射学到了知识，提升了神经网络的表现。</li>
<li>如果数字化和智能化探索无果，就止损守成 —— 残差映射没学到东西，不干了，让残差连接主导，保持住神经网络的表现。</li>
</ul>
<h2 id="四效果如何">四、效果如何？</h2>
<p>研究者实践发现，残差块有效地改善了深层神经网络的学习过程，提升了它们的性能。这让构建更深度的神经网络（比如百层甚至千层）成为可能。</p>
<p>因此，残差块这个设计理念对后续深度神经网络的构建产生了深远的影响。</p>

        </div>
        

        


        
        
        <div style="height: 50px;"></div>
        
        

        <div class="site-footer">
  
  
</div>

      </div>
    </div>
  </article>

  <script src="/js/highlight.pack.js"></script>


<script>
  hljs.initHighlightingOnLoad();
  
  
  
    
    
  
</script>




<script type="text/javascript"
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({ tex2jax: {inlineMath: [["$","$"],["\\(","\\)"]]} })
</script>

  

</body>

</html>