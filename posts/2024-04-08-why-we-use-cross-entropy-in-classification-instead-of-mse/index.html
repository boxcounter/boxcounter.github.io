<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="Hugo 0.126.1">

  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="author" content="Boxcounter" />
  <meta property="og:url" content="http://boxcounter.com/posts/2024-04-08-why-we-use-cross-entropy-in-classification-instead-of-mse/" />
  <link rel="canonical" href="http://boxcounter.com/posts/2024-04-08-why-we-use-cross-entropy-in-classification-instead-of-mse/" /><script type="application/ld+json">
  {
      "@context" : "http://schema.org",
      "@type" : "BlogPosting",
      "mainEntityOfPage": {
           "@type": "WebPage",
           "@id": "http:\/\/boxcounter.com\/"
      },
      "articleSection" : "posts",
      "name" : "为什么 classification 要用 cross entropy 来表达 loss，而不像 regression 那样用 MSE？",
      "headline" : "为什么 classification 要用 cross entropy 来表达 loss，而不像 regression 那样用 MSE？",
      "description" : "在读《Dive into Deep Learning》关于 regression 的章节时，我产生了这个疑问，思索查询后整理成了这篇博文。 原文在这里：Loss Function 原因之一： classification 的 predicted label 和",
      "inLanguage" : "en-US",
      "author" : "Boxcounter",
      "creator" : "Boxcounter",
      "publisher": "Boxcounter",
      "accountablePerson" : "Boxcounter",
      "copyrightHolder" : "Boxcounter",
      "copyrightYear" : "2024",
      "datePublished": "2024-04-08 00:00:00 \u002b0000 UTC",
      "dateModified" : "2024-04-08 00:00:00 \u002b0000 UTC",
      "url" : "http:\/\/boxcounter.com\/posts\/2024-04-08-why-we-use-cross-entropy-in-classification-instead-of-mse\/",
      "keywords" : [  ]
  }
</script>
<title>为什么 classification 要用 cross entropy 来表达 loss，而不像 regression 那样用 MSE？ - Boxcounter&#39;s blog</title>
  <meta property="og:title" content="为什么 classification 要用 cross entropy 来表达 loss，而不像 regression 那样用 MSE？ - Boxcounter&#39;s blog" />
  <meta property="og:type" content="article" />
  <meta name="description" content="在读《Dive into Deep Learning》关于 regression 的章节时，我产生了这个疑问，思索查询后整理成了这篇博文。 原文在这里：Loss Function 原因之一： classification 的 predicted label 和" />

  <link rel="stylesheet" href="/css/flexboxgrid-6.3.1.min.css" />
  <link rel="stylesheet"
    href="/css/github-markdown.min.css" />
  <link rel="stylesheet" href="/css/highlight/tomorrow.min.css" />
  <link rel="stylesheet" href="/css/index.css">
  <link href="/index.xml" rel="alternate" type="application/rss+xml" title="Boxcounter&#39;s blog">
  
  <link href="https://fonts.googleapis.com/css?family=Arvo|Permanent+Marker" rel="stylesheet">
  
  

  
</head>


<body>
  <article class="post " id="article">
    <div class="row">
      <div class="col-xs-12 col-sm-10 col-md-8 col-sm-offset-1 col-md-offset-2 col-lg-6 col-lg-offset-3">
        <div class="site-header">
          
<header>
  <div class="signatures site-title">
    <a href="/">Boxcounter</a>
  </div>
</header>
<div class="row end-xs">
  
  
</div>
<div class="header-line"></div>

        </div>
        <header class="post-header">
          <h1 class="post-title">为什么 classification 要用 cross entropy 来表达 loss，而不像 regression 那样用 MSE？</h1>
          
          <div class="row post-desc">
            <div class="col-xs-6">
              
              <time class="post-date" datetime="2024-04-08 00:00:00 UTC">
                08 Apr 2024
              </time>
              
            </div>
            <div class="col-xs-6">
              
              <div class="post-author">
                <a target="_blank" href="http://boxcounter.com/">@Boxcounter</a>
              </div>
              
            </div>
          </div>
          
        </header>

        <div class="post-content markdown-body">
          <p>在读《Dive into Deep Learning》关于 regression 的章节时，我产生了这个疑问，思索查询后整理成了这篇博文。</p>
<p>原文在这里：<a href="https://d2l.ai/chapter_linear-classification/softmax-regression.html#loss-function">Loss Function</a></p>
<p>原因之一：</p>
<p>classification 的 predicted label 和 real label 都是概率分布，误差也用概率表达最为自然。我们期望 predicted label 更“极端”、更接近 real label 的 one hot（它就很极端——只有一个值是 1，其余都是 0），数值平均和这个期望背道而驰。</p>
<p>原因之二：</p>
<p>Cross entropy 和 softmax 更般配、节省计算复杂度。前者是对数计算，后者是指数计算。简直天作之合。</p>
<p>原因之三：</p>
<p>用 MSE 的计算方法会“稀释” loss，降低梯度下降的速度。以一个 binary classification 为例：</p>
<ul>
<li>True label 是 [1, 0]</li>
<li>Predicted label 是 [0.01, 0.99]。</li>
</ul>
<p>可以很直观的看出这个预测错得离谱。我们分别用 MSE 和 Cross entropy 来算一算。</p>
<ul>
<li>MSE = $((1-0.01)^2 + (0-0.99)^2)/2$ = 0.49005</li>
<li>Cross Entropy loss = $-(1*\log 0.01 + 0*\log 0.99)$ = 2.0</li>
</ul>
<p>从结果可以看出，Cross entropy loss 要比 MSE 大得多，这也就让梯度下降更快，模型优化速度更快。</p>

        </div>
        

        


        
        
        <div style="height: 50px;"></div>
        
        

        <div class="site-footer">
  
  
</div>

      </div>
    </div>
  </article>

  <script src="/js/highlight.pack.js"></script>


<script>
  hljs.initHighlightingOnLoad();
  
  
  
    
    
  
</script>




<script type="text/javascript"
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({ tex2jax: {inlineMath: [["$","$"],["\\(","\\)"]]} })
</script>

  

</body>

</html>